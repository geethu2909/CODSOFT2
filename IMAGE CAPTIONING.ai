from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

model = ResNet50(weights='imagenet')

img_path = 'path_to_image.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

features = model.predict(x)
from tensorflow.keras.models import Model

feature_extractor = Model(inputs=model.input, outputs=model.layers[-2].output)
features = feature_extractor.predict(x)
import json

with open('captions.json', 'r') as f:
    captions = json.load(f)

# Example caption data format: { "image_id": "caption" }
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(captions.values())
sequences = tokenizer.texts_to_sequences(captions.values())
max_length = max(len(seq) for seq in sequences)
vocab_size = len(tokenizer.word_index) + 1
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

image_input = Input(shape=(2048,))
img_dense = Dense(256, activation='relu')(image_input)

text_input = Input(shape=(max_length,))
text_embed = Embedding(vocab_size, 256, mask_zero=True)(text_input)
text_lstm = LSTM(256)(text_embed)

decoder = add([img_dense, text_lstm])
decoder = Dense(vocab_size, activation='softmax')(decoder)

model = Model(inputs=[image_input, text_input], outputs=decoder)
model.compile(loss='categorical_crossentropy', optimizer='adam')
from transformers import TFBertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

image_input = Input(shape=(2048,))
img_dense = Dense(768, activation='relu')(image_input)

text_input = Input(shape=(max_length,))
text_embeddings = bert_model(text_input)[0]

decoder = add([img_dense, text_embeddings])
decoder = Dense(vocab_size, activation='softmax')(decoder)

model = Model(inputs=[image_input, text_input], outputs=decoder)
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit([features, sequences], epochs=20, batch_size=64)
def generate_caption(image, model, tokenizer, max_length):
    features = feature_extractor.predict(image)
    input_seq = [tokenizer.word_index['startseq']]
    for _ in range(max_length):
        sequence = pad_sequences([input_seq], maxlen=max_length)
        yhat = model.predict([features, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = tokenizer.index_word[yhat]
        if word == 'endseq':
            break
        input_seq.append(yhat)
    return ' '.join([tokenizer.index_word[i] for i in input_seq if i > 0])

caption = generate_caption(x, model, tokenizer, max_length)
print(caption)
